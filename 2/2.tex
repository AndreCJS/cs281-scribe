\documentclass{article}
\usepackage[margin=1in]{geometry}
\usepackage{../common}
\usepackage{../pagesetup}

\begin{document}
%\lecture{**LECTURE-NUMBER**}{**DATE**}{**LECTURER**}{**SCRIBE**}
\lecture{1}{September 6}{Sasha Rush}{Anna Sophie Hilgard}{Discrete Models}

\begin{itemize}
\item Take values from a countable set, e.g. \{0,1\}, \{cold, flu, asthma\}
\item We will use simple discrete models to develop our tactics such as marginalization and conditioning. Today: coins.
\end{itemize}

\subsection{Bernoulli model}
The likelihood is of the form $p(\text{heads}) = \theta$.

\subsubsection*{Easy Prior}
Assume we know the coin came from one of 3 unknown manufacturers (Later, we'll have mixture model estimation, but for now assume these probabilities come from an oracle.)\\
$\theta = 0.4$ with probability .1\\
$\theta = 0.5$ with probability .8\\
$\theta = 0.6$ with probability .1

$p(\theta) = 0.1 \cdot \delta(\theta = 0.4)  + 0.8 \cdot \delta(\theta = 0.5) + 0.1 \cdot \delta(\theta = 0.6)$


\subsubsection*{Likelihood}
Likelihood: $p(\text{data}|\text{parameters})$

Ex: $p(\text{coin\_flips}|\theta) = \textrm{Bin}(N_1 | N, \theta) = {N \choose N_1} \theta^{N_1} (1-\theta)^{N-N_1}$\\
Where $N = N_0 + N_1 =$ number of flips\\
Note that the last term, the ``score", is the only term that depends on $\theta$.

\subsection{Inference}
Inference 1: $p(\theta|x)$ ($x = N_0, N_1$)

\subsubsection*{Maximum Likelihood Estimation (MLE)}
$\theta_{MLE} = \textrm{argmax}_{\theta} \; p(N_0,N_1 | \theta) = \textrm{argmax}_{\theta}  \;\textrm{log}\left[p(N_0,N_1 | \theta)\right]$

$\theta_{MLE} = \textrm{argmax}_{\theta} \;\textrm{log}{N \choose N_1} + N_1 \textrm{log} \theta + N_0  \textrm{log} (1-\theta)$

Because the first term is not a function of $\theta$, we can ignore it.

$\frac{d}{d\theta} = \frac{N_1}{\theta} + \frac{N_0}{1-\theta} \cdot ( -1)  \rightarrow \theta_{MLE} = \frac{N_1}{N_0 + N_1}$

\begin{itemize}
\item Note that Inference != Decision Making. If we asked you to make a bet on the coin, based on this you could either
\begin{enumerate}
\item Always take heads if $\theta > .5$. In this case, $p(win) = \theta$
\item Take heads with probability = $\theta$. In this case, $p(win) = \theta^2 + (1-\theta)^2$ [p(is heads) * p(choose heads) + ...]
\end{enumerate}
For $\theta = .6$, the win probabilities are .6 and .52, respectively. 
\end{itemize}

\subsubsection*{Maximizing the Posterior (MAP)}
Bayes Rule : $p(\theta|data) \propto p(data|\theta) p(\theta)$
\begin{itemize}
\item Posterior: $p(\theta | x)$
\item Likelihood: $p(x | \theta)$
\item Prior: $p(\theta)$
\end{itemize}
$\theta_{MAP} = \textrm{argmax}_{\theta} \; p(\theta | x) = \textrm{argmax}_{\theta}  \;\textrm{log}\left[p(x | \theta) p(\theta)\right]$\\

Ex:\\
$p(\theta=0.4|N_0,N_1) \propto {N \choose N_1} (.4)^{N_1}(1-.4)^{N_0} (0.1)$

$p(\theta=0.45|N_0,N_1) = 0 $ (Due to the sparsity of the prior)\\

MAP = MLE when we have a uniform prior

\subsubsection*{Full Posterior}

\begin{itemize}
\item Partition/Marginal: $p(N_0,N_1) = \int_\theta p(N_0,N_1,\theta)$. Note this is evil and hard to compute
\end{itemize}
$p(\theta|N_0,N_1) = \frac{p(x|\theta) p(\theta)}{p(N_0,N_1)}$

\subsubsection*{Beta Prior}

Beta Distribution: $p(\theta|\alpha_0,\alpha_1) = \frac{\Gamma(\alpha_0 + \alpha_1)}{\Gamma(\alpha_0)\Gamma(\alpha_1)} \theta^{\alpha_1-1} (1-\theta)^{\alpha_0-1}$ 

From the image of the beta function for different parameters, we can see that it can ether be balanced, humped to one side, or tend toward infinity on one side or the other.
\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{./beta.png}
\caption{Beta Params}
\end{figure}

With a beta prior:  $p(\theta|N_0,N_1) = \frac{\Gamma(\alpha_0 + \alpha_1)}{\Gamma(\alpha_0)\Gamma(\alpha_1)} \theta^{\alpha_1-1} (1-\theta)^{\alpha_0-1} \theta^{N_1}(1-\theta)^{N_0} \cdot \textrm{(constant normalizer w.r.t} \theta) $

The key insight is that we get additive terms in the exponent and the resulting distribution looks like another beta. The prior ``counts'' (pseudocounts) from the hyperparameters can be interpreted as counts we have beforehand

$p(\theta|N_0,N_1) = \frac{\Gamma(\alpha_0 + \alpha_1)}{\Gamma(\alpha_0)\Gamma(\alpha_1)} \theta^{N_1 + \alpha_1-1} (1-\theta)^{N_0 + \alpha_0-1} \cdot \textrm{(constant)}$ 

To make this distribution sum to 1, use the known beta normalizer

$p(\theta|N_0,N_1) = \frac{\Gamma(\alpha_0 + \alpha_1 + N_0 + N_1)}{\Gamma(\alpha_0)\Gamma(\alpha_1)\Gamma(N_0)\Gamma(N_1)} \theta^{N_1 + \alpha_1-1} (1-\theta)^{N_0 + \alpha_0-1} $ 

$p(\theta|N_0,N_1) \sim \textrm{Beta}(\theta | N_0 + \alpha_0, N_1 + \alpha_1)$ is the posterior distribution.

\begin{itemize}
\item The mode of the Beta gives us back the MAP, but we additionally now have information about the shape.
\item What does the prior that tends to infinity at 1 imply? That in the absence of other information, the coin is definitely heads. 
\end{itemize}

\subsubsection*{Predictive Distribution}

$p(\hat{x}| N_0, N_1) = \int_\theta p(x|\theta, N_0, N_1) p(\theta | N_0, N_1) d\theta$

$p(\hat{x}| N_0, N_1) = \int_\theta \theta p(\theta | N_0, N_1) d\theta$

$p(\hat{x}| N_0, N_1) = \E_{\theta \sim p(\theta|N_0, N_1)} \theta$ (The expectation under the posterior of $\theta$. This is the mean of the Beta distribution. Feel free to prove it as an exercise!)

\subsubsection*{Normalizing Term / Marginal (Likelihood)}
$p(N_0, N_1) = \int_\theta p(x_1, \hdots x_n | \theta) p(\theta) d\theta$ \\
$p(N_0, N_1) = \int_\theta \frac{\Gamma( \alpha_1 +  \alpha_1)}{\Gamma(\alpha_0)\Gamma(\alpha_1)} \theta^{\alpha_1 + N_1 -1} (1-\theta)^{\alpha_0+N_0-1} $

The first term can be moved outside, as it does not depend on $\theta$. Then we know what the integral of the rest should be because we know how to make the distribution sum to 1.

$p(N_0, N_1) = \frac{\Gamma( \alpha_1 +  \alpha_1)}{\Gamma(\alpha_0)\Gamma(\alpha_1)} \frac{\Gamma(N_0 + \alpha_0)\Gamma(N_1 + \alpha_1}{\Gamma(N_0 + N_1 + \alpha_0 + \alpha_1}$

\subsection{Extensions on the Coin Flip Model: Super Coins}
\begin{itemize}
\item Many coins and they're correlated (Models of binary data)
\item Many-sided coins aka dice (Models of categorical data)
\end{itemize}

\subsubsection*{Bernoulli}
$\textrm{Ber}(x|\theta) = \theta^{x}(1-\theta)^x$
\subsubsection*{Categorical}
$\textrm{Cat}(x|\theta) = \prod_k\theta_k^{x_k}$
\subsubsection*{Multinomial}
$\textrm{Multi}(x|\theta) = \frac{(\sum x_k) !}{\prod_k x_k!}\prod_k\theta_k^{x_k}$
\subsubsection*{Dirichlet}
$\textrm{Dir}(x|\alpha) = \frac{\Gamma(\sum \alpha_k)}{\prod \Gamma(\alpha_k)}\prod_k\theta_k^{\alpha_k-1}$
% **** THIS ENDS THE EXAMPLES. DON'T DELETE THE FOLLOWING LINE:

\subsection{Example notebook}
See \href{https://github.com/harvard-ml-courses/cs281-demos/blob/master/Beta.ipynb}{Beta.ipynb}
\end{document}